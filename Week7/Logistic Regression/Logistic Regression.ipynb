{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic Regression\n",
    "\n",
    "_Tham khảo thêm:_\n",
    "\n",
    "https://ml-cheatsheet.readthedocs.io/en/latest/logistic_regression.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Introduction\n",
    "\n",
    "`Logistic regression` được dùng cho bài toán phân loại nhị phân (`binary classification`). `Ouput` (đầu ra) của logistic regression là giá trị kiểu float nằm trong khoảng `[0,1]` và có thể được dùng để phân loại nhị phân. Ví dụ chúng ta muốn xây dựng mô hình logistic regression để xác định ảnh đầu vào có chứa con mèo hay không. Lúc này ảnh chứa con mèo có thể xác định bằng cách so sánh giá trị output với một giá trị T (thường cho T = 0.5). Nếu output > T, mô hình quyết định ảnh input có chứa con mèo. Ngược lại, ảnh input không chứa con mèo.\n",
    "\n",
    "**_So sánh với Linear Regression_**\n",
    "\n",
    "Giả sử có bộ dữ liệu về điểm bài kiểm tra của sinh viên, đều laf input của Linear Regression và Logistic Regression. Sự khác nhau thể hiện ở:\n",
    "\n",
    "- `Linear Regression`: dự đoán số điểm của sinh viên trong khoảng [0,100]. Linear Regression dự đoán cho hàm liên tục. \n",
    "- `Logistic Regression`:  dự đoán liệu rằng sinh viên có đỗ hay trượt. Logistic Regression dự đoán rời rạc.\n",
    "\n",
    "**_Type of Logistic Regression_**\n",
    "\n",
    "- Binary(Pass/Fail)\n",
    "- Multi (Cats, Dogs, Sheep)\n",
    "- Ordinal(Low, Medium, High)\n",
    "\n",
    "### Binary Logistic Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**_Sigmoid activation_**\n",
    "\n",
    "Hàm `Sigmoid` được sue dụng tỏng trường hợp dự đoán theo xác suất. Hàm này ánh xạ bất kỳ một giá trị thực nào thành giá trị khác trong khoảng [0,1]. Trong ML, sử dụng `Sigmoid` để ánh xạ dự doán theo xác suất.\n",
    "\n",
    "$$ S(z) = \\frac{1}{1+e^{-z}}$$\n",
    "\n",
    "_Note:_\n",
    "- s(z)  = output between 0 and 1 (probability estimate)\n",
    "- z = input to the function (your algorithm’s prediction e.g. mx + b)\n",
    "- e = base of natural log\n",
    "\n",
    "_Graph:_\n",
    "![sigmoid.png](img/sigmoid.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def sigmoid(z, derivative=False):\n",
    "    sigm = 1. /(1. + np.exp(-z))\n",
    "    if derivative:\n",
    "        return sigm*(1-sigm)\n",
    "    return sigm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**_Decision boundary_**\n",
    "\n",
    "Hàm dự đoán sẽ trả về một điểm xác suất trong khoảng  [0,1]. Để ánh xạ nó tới một lớp riêng biệt (đúng/sai, mèo/chó). Chúng ta sẽ chọn một giá trị ngưỡng (threshold) và phân loại các giá trị vào lớp 1 hoặc lớp 2. Ví dụ threshold = 0.5\n",
    "\n",
    "$$ p >= 0.5 , class = 1 $$\n",
    "$$ p <= 0.5 , class = 0 $$\n",
    "\n",
    "![logistic_regression_sigmoid_w_threshold.png](img/logistic_regression_sigmoid_w_threshold.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**_Making predictions_**\n",
    "\n",
    "Sử dụng `Sigmoid` và `Decison boundary`, ta sẽ tạo hàm `prediction function` . Hàm `prediction` trả về xác suất cho sự học tạp là dương, True hoặc 'Yes'. Ta gọi class này là 1 và kí hiệu: $P(class=1)$.\n",
    "\n",
    "$$ z = \\theta^{T}x$$\n",
    "$$ P(class=1) = \\frac{1}{1 + e^{-z}}$$\n",
    "\n",
    "Nếu mô hình trả về 0.4 tức là 40% cơ hội 'Pass', nếu threshold = 0.5 thì phân loại là 'Fail'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(features, weights):\n",
    "    '''\n",
    "    Return 1D array of probabilities\n",
    "    that the class label == 1\n",
    "    '''\n",
    "    \n",
    "    z = np.dot(features, weights)\n",
    "    return sigmoid(z)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**_Cost function_**\n",
    "\n",
    "Thay vì sử dụng hàm Mean Squared Error, chúng ta sử dụng hàm Cross-Entropy, hay được goị là Log Loss. Cross-Entropy được chia làm 2 phần cost functions: `y=1` và `y=0`.\n",
    "\n",
    "$$ J(\\theta) = \\frac{1}{m}\\sum^{m}_{i=1}Cost(h_\\theta(x^{(i)}), y^{(i)})$$\n",
    "$Cost(h_\\theta(x), y) = -log(h_\\theta(x))$   if y = 1\n",
    "\n",
    "$Cost(h_\\theta(x), y) = -log(1 - h_\\theta(x))$    if y = 0\n",
    "\n",
    "![y1andy2_logistic_function.png](img/y1andy2_logistic_function.png)\n",
    "\n",
    "**Hàm cost cụ thể**\n",
    "![logistic_cost_function_joined.png](img/logistic_cost_function_joined.png)\n",
    "\n",
    "**Vectorized cost function**\n",
    "![logistic_cost_function_joined.png](img/logistic_cost_function_vectorized.png)\n",
    "\n",
    "Code:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cost_function(features, labels, weights):\n",
    "    '''\n",
    "    Using Mean Absolute Error\n",
    "    \n",
    "    Returns 1D matrix of predictions\n",
    "    Cost = (labels*log(predictions) + (1-labels)*log(1-predictions) ) / len(labels)\n",
    "    '''\n",
    "    observations = len(labels)\n",
    "    \n",
    "    predictions = predict(features, weights)\n",
    "    \n",
    "    #take the error when label=1\n",
    "    class1_cost = -labels*np.log(predictions)\n",
    "    \n",
    "    #take the error when label=0\n",
    "    class0_cost = (1-labels)*np.log(1-predictions)\n",
    "    \n",
    "    #take the sum of both costs\n",
    "    cost = class1_cost + class0_cost\n",
    "    \n",
    "    #take the average cost\n",
    "    cost = cost.sum()/observations\n",
    "    \n",
    "    return cost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**_Gradien Descent_**\n",
    "\n",
    "Gradient Descent được dùng để minimize giá trị hàm cost.\n",
    "\n",
    "$$ s'(z) = s(z)(1-s(z))$$\n",
    "$$ C' = x(s(z) - y) $$\n",
    "\n",
    "_Note_\n",
    "\n",
    "- C′ is the derivative of cost with respect to weights\n",
    "- y is the actual class label (0 or 1)\n",
    "- s(z) is your model’s prediction\n",
    "- x is your feature or feature vector.\n",
    "\n",
    "Repeat {\n",
    "\n",
    "    1. Calculate Gradient average\n",
    "    2. Multiply by learning rate\n",
    "    3. Subtract from weights\n",
    "}\n",
    "\n",
    "Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_weights(features, labels, weights, lr):\n",
    "    '''\n",
    "    Vectorized Gradient Descent\n",
    "    '''\n",
    "    N = len(features)\n",
    "    \n",
    "    #1. Get Predictions\n",
    "    predictions = predict(features, weights)\n",
    "    \n",
    "    #2. Tranpose features so we can multiply w cost matrix.\n",
    "    gradient = np.dot(features.T, predictions-labels)\n",
    "    \n",
    "    #3. Take the average cost derivative for each feature\n",
    "    gradient /= N\n",
    "    \n",
    "    #4. Multiplt the gradient by our learning rate\n",
    "    gradien *=lr\n",
    "    \n",
    "    #5. Subtract from our weights to minimize cost\n",
    "    weights -= gradient\n",
    "    \n",
    "    return weights\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**_Mapping probabilities to classes_**\n",
    "\n",
    "Bước cuối cùng là phân nhã cho dự đoán xác suất tính được\n",
    "\n",
    "- **Decision boundary**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decision_boundary(prob):\n",
    "    return 1 if prob >= 0.5 else 0\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Covert probabilities to classes**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def classify(predictions):\n",
    "    \n",
    "    '''\n",
    "    input = N element array of predictions between 0 and 1\n",
    "    output = N element array of 0s(False) and 1s(True)\n",
    "    '''\n",
    "    \n",
    "    decision_boundary = np.vectorize(decision_boundary)\n",
    "    \n",
    "    return decision_boundary(predictions).flatten()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**_Training_**\n",
    "\n",
    "Code phần training tương tự với Linear Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(features, labels, weights, iters):\n",
    "    cost_history = []\n",
    "    \n",
    "    for i in range(inters):\n",
    "        weights = update_weights(features, labels, weights, lr)\n",
    "        \n",
    "        #calculate erroe for additing purposes\n",
    "        cost = cost_function(features, labels, weights)\n",
    "        cost_function.append(cost)\n",
    "        \n",
    "        #log progress\n",
    "        if i % 1000 == 0:\n",
    "            print(\"iter: \"+ str(i) + \"cost: \" + str(cost))\n",
    "            \n",
    "    return weights, cost_history"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multiclass Logistic Regression\n",
    "\n",
    "Thay vì y = 0, 1 thì y được mở rộng ở nhiều biến y = 0, 1, ..., n\n",
    "\n",
    "**Procedure**\n",
    "    1. Coi bài toán là bài toán phân loại nhị phân n+1 (+1 vì chỉ số bắt đầu từ 0)\n",
    "    2. Thực hiện với mỗi class..\n",
    "    3. Dự đoán xác suất của từng bản ghi với mỗi lớp đơn\n",
    "    4. Kết luận: prediction = <math>max(probability of classes)\n",
    "    \n",
    "**Softmax activation**\n",
    "\n",
    "**Scikit-Learn exapmle**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.cross_validation import train_test_split\n",
    "\n",
    "# Normalize grades to values between 0 and 1 for more efficient computation\n",
    "normalized_range = sklearn.preprocessing.MinMaxScaler(feature_range=(-1,1))\n",
    "\n",
    "# Extract Features + Labels\n",
    "labels.shape =  (100,) #scikit expects this\n",
    "features = normalized_range.fit_transform(features)\n",
    "\n",
    "# Create Test/Train\n",
    "features_train,features_test,labels_train,labels_test = train_test_split(features,labels,test_size=0.4)\n",
    "\n",
    "# Scikit Logistic Regression\n",
    "scikit_log_reg = LogisticRegression()\n",
    "scikit_log_reg.fit(features_train,labels_train)\n",
    "\n",
    "#Score is Mean Accuracy\n",
    "scikit_score = clf.score(features_test,labels_test)\n",
    "print 'Scikit score: ', scikit_score\n",
    "\n",
    "#Our Mean Accuracy\n",
    "observations, features, labels, weights = run()\n",
    "probabilities = predict(features, weights).flatten()\n",
    "classifications = classifier(probabilities)\n",
    "our_acc = accuracy(classifications,labels.flatten())\n",
    "print 'Our score: ',our_acc"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
